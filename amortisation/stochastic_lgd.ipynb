{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77c9492",
   "metadata": {},
   "source": [
    "# Monte Carlo Default Simulation\n",
    "\n",
    "We aim to estimate the expected loss up to time T:\n",
    "\n",
    "$$E[L_T] = E\\left[\\sum_{i=1}^{N} \\ell_i \\cdot 1\\{T_i \\leq T\\}\\right]$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\ell_i$ represents the loss amount from default $i$\n",
    "- $T_i$ represents the default time for firm $i$\n",
    "- $1\\{T_i \\leq T\\}$ is the indicator function for defaults occurring before time $T$\n",
    "\n",
    "## Model Components\n",
    "\n",
    "### Firm Intensity Process\n",
    "Each firm's default intensity is modeled as:\n",
    "\n",
    "$$\\lambda_t^i = X_i + \\sum_{j=1}^{J} w_{ij} Y_t^j$$\n",
    "\n",
    "Where:\n",
    "- $X_i$ is the idiosyncratic component for firm $i$\n",
    "- $Y_t^j$ is the sectoral intensity component\n",
    "- $w_{ij}$ are loading factors representing sensitivity to sectoral factors\n",
    "\n",
    "### Sectoral Intensity \n",
    "\n",
    "**We assume orthogonal sectors.**\n",
    "\n",
    "$$dY_t^j = \\kappa_j(\\theta_j - Y_t^j)dt + \\sigma_{j}\\sqrt{Y_t^j}dW_t^j + \\sum_{i=1}^{N}\\delta_{ij}dL_t^i$$\n",
    "\n",
    "Where:\n",
    "- $\\kappa_j$ is the mean reversion speed\n",
    "- $\\theta_j$ is the long-term mean level\n",
    "- $\\sigma_{j}$ are volatility parameters\n",
    "- $W_t^j$ are standard Brownian motions\n",
    "- $\\delta_{ij}$ are contagion coefficients\n",
    "- $L_t$ is the loss process \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322819db",
   "metadata": {},
   "source": [
    "## We experiment with integrating amortisation schedules in the above intensity based model.\n",
    "\n",
    "### Hence the loss becomes LGD times the Exposure-at-Default.\n",
    "\n",
    "We consider four repayment schedules. \n",
    "1) \"Bullet\" repayment where the outstanding principal is repaid in full at the time of \"maturity\".\n",
    "\n",
    "2) \"Linear/Italian\" repayment scheme characterised by a constant repayment of principal in each installment.\n",
    "\n",
    "3) \"French\" repayment scheme where a constant total installment (Principal + Interest) throughout the life of the loan is paid.\n",
    "\n",
    "4) \"Negative Amortisation\" where the obligor continues to borrow and accrue further interest on the sum owed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546e3960",
   "metadata": {},
   "source": [
    "## In this notebook, instead of using plain exposure as the loss marked (i.e. LGD = 1), we take LGD $\\sim$ Beta($\\alpha$, $\\beta$)\n",
    "\n",
    "$\\alpha$, $\\beta$ parameters calculated using LGD_MEAN = 0.45, LGD_STDDEV = 0.15 (in accordance with regularoty assumptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fef04a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import ncx2\n",
    "from scipy.special import hyp1f1\n",
    "from scipy.optimize import brentq\n",
    "from math import exp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "DECIMALS = 6\n",
    "\n",
    "np.set_printoptions(precision=DECIMALS, suppress=True)\n",
    "pd.options.display.float_format = lambda x: f\"{x:.{DECIMALS}f}\"\n",
    "pd.set_option(\"display.precision\", DECIMALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d991c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_max_generator(epsilon, y, theta, sigma, kappa, max_attempts=5, tol=1e-8):\n",
    "    \"\"\"\n",
    "    \n",
    "    Compute H^*_epsilon such that P_y(sigma_H < tau) = G_y(H; H) <= epsilon.\n",
    "    This function implements Equation (11) using the closed-form in Equation (19).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def G_y(H):\n",
    "\n",
    "        a = H / kappa  # Now using H as the Laplace exponent (since tau ~ Exp(H))\n",
    "        b = 2 * kappa * theta / sigma**2\n",
    "        z_y = 2 * kappa * y / sigma**2\n",
    "        z_H = 2 * kappa * H / sigma**2\n",
    "\n",
    "        num = hyp1f1(a, b, z_y)\n",
    "        denom = hyp1f1(a, b, z_H)\n",
    "        \n",
    "        return num / denom  # This is G_y(H; H) = E[e^{-H * sigma_H}]\n",
    "\n",
    "    def root_function(H):\n",
    "        return G_y(H) - epsilon\n",
    "\n",
    "    # Bracketing interval\n",
    "    H_min = y + 1e-8\n",
    "    H_max = y + 10.0\n",
    "\n",
    "    # Expand H_max until G_y(H_max) < epsilon\n",
    "    for _ in range(max_attempts):\n",
    "        try:\n",
    "          H_star = scipy.optimize.toms748(root_function, H_min, H_max, xtol=tol)\n",
    "          \n",
    "          if root_function(H_star)<=0:\n",
    "            \n",
    "            return H_star\n",
    "          \n",
    "          else:\n",
    "          \n",
    "            while root_function(H_star)>0:\n",
    "              H_star = H_star + 1e-2\n",
    "          \n",
    "            return H_star\n",
    "          \n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        H_max += 5.0\n",
    "\n",
    "    # Fallback: scan manually to find conservative H\n",
    "    print(\"[Warning] brentq failed to converge. Using fallback grid search.\")\n",
    "    H_vals = np.linspace(H_min, H_max + 50, 1000)\n",
    "    for H in H_vals:\n",
    "        if G_y(H) <= epsilon:\n",
    "            return H\n",
    "\n",
    "    raise RuntimeError(\"Unable to find H^*_epsilon. Try expanding search space.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8109edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cir_transition_sample_per_sector(y_vec, tau, kappa_vec, theta_vec, sigma_vec, rng):\n",
    "    \"\"\"\n",
    "    y_vec, kappa_vec, theta_vec, sigma_vec are arrays of length J (sectors).\n",
    "    Returns xi_vec: sampled Y_{t+tau} per sector (length J).\n",
    "    \"\"\"\n",
    "    if tau <= 0:\n",
    "        return y_vec.copy()\n",
    "    \n",
    "    one_minus = -np.expm1(-kappa_vec * tau)  # = 1 - exp(-kappa*tau)\n",
    "    # avoid zeros\n",
    "    one_minus = np.where(one_minus <= 0, 1e-16, one_minus)\n",
    "    \n",
    "    c = (sigma_vec * sigma_vec * one_minus) / (4.0 * kappa_vec)\n",
    "    d = 4.0 * kappa_vec * theta_vec / (sigma_vec * sigma_vec)\n",
    "    \n",
    "    # noncentrality parameters\n",
    "    nc = (4.0 * kappa_vec * np.exp(-kappa_vec * tau) * y_vec) / (sigma_vec * sigma_vec * one_minus)\n",
    "    \n",
    "    # guard\n",
    "    d = np.maximum(d, 1e-12)\n",
    "    nc = np.maximum(nc, 0.0)\n",
    "    \n",
    "    # sample per sector (simple looping works fine since J would be typically small)\n",
    "    xi = np.empty_like(y_vec, dtype=float)\n",
    "\n",
    "    for j in range(len(y_vec)):\n",
    "    \n",
    "        # If df or nc are extreme, ncx2.rvs might throw an error\n",
    "        try:\n",
    "            Z = ncx2.rvs(df=d[j], nc=nc[j], random_state=rng)\n",
    "    \n",
    "        except Exception:\n",
    "            Z = ncx2.rvs(df=max(d[j],1e-6), nc=0.0, random_state=rng) + nc[j]\n",
    "    \n",
    "        xi[j] = c[j] * Z\n",
    "    \n",
    "    xi = np.maximum(xi, 0.0)\n",
    "    \n",
    "    return xi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17e36edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LGD_MEAN = 0.45\n",
    "LGD_STD = 0.15\n",
    "\n",
    "def get_beta_params(mu, sigma):\n",
    "    if sigma**2 >= mu * (1 - mu):\n",
    "        raise ValueError(\"Standard deviation is too high for this mean.\")\n",
    "\n",
    "    nu = (mu * (1 - mu) / sigma**2) - 1\n",
    "    alpha = mu * nu\n",
    "    beta = (1 - mu) * nu\n",
    "    return alpha, beta\n",
    "\n",
    "alpha_lgd, beta_lgd = get_beta_params(LGD_MEAN, LGD_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59857443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.500000000000001, 5.500000000000002)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_lgd, beta_lgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de489028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def loss_distribution_plot(Payoff_T,index):\n",
    "  # Get histogram data (without plotting)\n",
    "  counts, bin_edges = np.histogram(Payoff_T, bins=10, density=True)\n",
    "\n",
    "  # Get bin centers\n",
    "  bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "  # Interpolation function\n",
    "  f_interp = interp1d(bin_centers, counts, kind='cubic', fill_value=\"extrapolate\")\n",
    "\n",
    "  # New x-values for smooth curve\n",
    "  x_smooth = np.linspace(bin_centers.min(), bin_centers.max(), 500)\n",
    "  y_smooth = f_interp(x_smooth)\n",
    "\n",
    "  # Plot histogram and interpolated curve\n",
    "  plt.hist(Payoff_T, bins=100, density=True, label='Histogram_'+str(index),alpha = 0.4)\n",
    "  plt.plot(x_smooth, y_smooth, label='Loss Distribution Case_'+str(index))\n",
    "  plt.xlabel('Loss')\n",
    "  plt.ylabel('Density')\n",
    "  # plt.legend()\n",
    "  plt.title('Loss Distribution for '+str(index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a4d000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_shortfall(losses, alpha=0.95):\n",
    "    var_alpha = np.quantile(losses, alpha)\n",
    "    tail_losses = losses[losses >= var_alpha]\n",
    "    return float(np.round(tail_losses.mean(), DECIMALS))\n",
    "\n",
    "def compute_metrics(losses, alpha=0.95):\n",
    "    var_alpha = np.quantile(losses, alpha)\n",
    "    es = expected_shortfall(losses, alpha)\n",
    "    metrics = {\n",
    "        \"Mean\": np.mean(losses),\n",
    "        \"Std\": np.std(losses),\n",
    "        \"VaR\": var_alpha,\n",
    "        \"ES\": es,\n",
    "        \"Excess ES\": es - var_alpha,\n",
    "        \"ExcessKurtosis\": kurtosis(losses)\n",
    "    }\n",
    "    return {key: float(np.round(value, DECIMALS)) for key, value in metrics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5afbfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_types = [\"bullet\", \"linear\", \"french\", \"negative\"]\n",
    "\n",
    "def bullet_exposure(contract, T, t):\n",
    "    P = float(contract[\"P\"])\n",
    "    return P if t < T else 0.0\n",
    "\n",
    "\n",
    "def linear_exposure(contract, T, t):\n",
    "    P = float(contract[\"P\"])\n",
    "    periods_per_year = float(contract[\"N\"])\n",
    "    if periods_per_year <= 0 or T <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    period = 1.0 / periods_per_year  # years per period\n",
    "    total_periods = int(np.ceil(T * periods_per_year))\n",
    "    payments_made = int(np.floor(t / period))\n",
    "    payments_made = int(np.clip(payments_made, 0, total_periods))\n",
    "    principal_paid = (payments_made / total_periods) * P\n",
    "    return max(P - principal_paid, 0.0)\n",
    "\n",
    "\n",
    "def french_exposure(contract, T, t):\n",
    "    P = float(contract[\"P\"])\n",
    "    periods_per_year = float(contract[\"N\"])\n",
    "    r_annual = float(contract[\"r\"])\n",
    "    if periods_per_year <= 0 or T <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    period = 1.0 / periods_per_year  # years per period\n",
    "    total_periods = int(np.ceil(T * periods_per_year))\n",
    "    payments_made = int(np.floor(t / period))\n",
    "    payments_made = int(np.clip(payments_made, 0, total_periods))\n",
    "    periods_remaining = total_periods - payments_made\n",
    "    if periods_remaining <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    r_period = r_annual / periods_per_year  # annual to per-period rate\n",
    "\n",
    "    if r_period == 0.0:\n",
    "        payment = P / total_periods\n",
    "        return payment * periods_remaining\n",
    "\n",
    "    annuity_payment = P * r_period / (1.0 - (1.0 + r_period) ** (-total_periods))\n",
    "    pv_remaining = annuity_payment * (1.0 - (1.0 + r_period) ** (-periods_remaining)) / r_period\n",
    "    return pv_remaining\n",
    "\n",
    "\n",
    "def negative_amortization_exposure(contract, T, t):\n",
    "    P = float(contract[\"P\"])\n",
    "    periods_per_year = float(contract[\"N\"])\n",
    "    r_annual = float(contract.get(\"r\", 0.0))\n",
    "    if periods_per_year <= 0 or T <= 0:\n",
    "        return max(P, 0.0)\n",
    "\n",
    "    period = 1.0 / periods_per_year  # years per period\n",
    "    total_periods = int(np.ceil(T * periods_per_year))\n",
    "    periods_elapsed = int(np.floor(t / period))\n",
    "    periods_elapsed = int(np.clip(periods_elapsed, 0, total_periods))\n",
    "\n",
    "    r_period = r_annual / periods_per_year  # annual to per-period rate\n",
    "\n",
    "    exposure = P * ((1.0 + r_period) ** periods_elapsed)\n",
    "\n",
    "    return exposure if t < T else 0.0\n",
    "\n",
    "\n",
    "def exposure_at_time(contract, T, t):\n",
    "    loan_type = str(contract[\"type\"]).lower()\n",
    "    dispatch = {\n",
    "        \"bullet\": bullet_exposure,\n",
    "        \"linear\": linear_exposure,\n",
    "        \"french\": french_exposure,\n",
    "        \"negative_amortisation\": negative_amortization_exposure,\n",
    "    }\n",
    "    if loan_type not in dispatch:\n",
    "        raise ValueError(f\"Unknown loan type: {loan_type}\")\n",
    "    return dispatch[loan_type](contract, T, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "120e1a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_sector_cir_model(kappa, theta, sigma, T, delta, W, eps, lambda_benchmark,\n",
    "                              Nfirms, idiosyncratic_factor, loan_contracts, alpha_lgd, beta_lgd, rng=None):\n",
    "\n",
    "    import warnings\n",
    "\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    # ensure shapes\n",
    "    J = len(theta)\n",
    "    assert W.shape == (Nfirms, J)\n",
    "    idiosyncratic_factor = np.asarray(idiosyncratic_factor).reshape(Nfirms,)\n",
    "    lambda_benchmark = np.asarray(lambda_benchmark).reshape(J,)\n",
    "\n",
    "    delta = np.asarray(delta)\n",
    "    if delta.ndim == 1:\n",
    "        delta = np.tile(delta.reshape(1, J), (Nfirms, 1))\n",
    "    assert delta.shape == (Nfirms, J)\n",
    "\n",
    "    # init\n",
    "    t = 0.0\n",
    "    Y_t = theta.copy().astype(float)   # sectoral intensities (J-vector), initialised at theta\n",
    "    events = []\n",
    "    owed = []\n",
    "    marks = []\n",
    "    defaulter_idio_factor = []\n",
    "    sector_contributions = []\n",
    "    alive = np.arange(Nfirms, dtype=int)\n",
    "\n",
    "    # precompute some arrays for speed\n",
    "    kappa = np.asarray(kappa, dtype=float)\n",
    "    sigma = np.asarray(sigma, dtype=float)\n",
    "    theta = np.asarray(theta, dtype=float)\n",
    "\n",
    "    # event loop\n",
    "    while (t < T) and (alive.size > 0):\n",
    "\n",
    "        lambda_max = np.maximum(Y_t, lambda_benchmark)  # J-vector\n",
    "        # Hepsilon = sum_i [ X_i + w_i Â· lambda_max ] over alive firms\n",
    "        \n",
    "        # We can compute per-firm value and sum\n",
    "        per_firm_sys = W[alive].dot(lambda_max)   # length alive.size\n",
    "        per_firm_idio = idiosyncratic_factor[alive]\n",
    "        Hepsilon = per_firm_sys.sum() + per_firm_idio.sum()\n",
    "\n",
    "        if Hepsilon <= 0:\n",
    "            break\n",
    "\n",
    "        # sample a candidate waiting time\n",
    "        tau = rng.exponential(1.0 / Hepsilon)\n",
    "        t_candidate = t + tau\n",
    "        if t_candidate >= T:\n",
    "            break\n",
    "\n",
    "        # sample sectoral Y at t_candidate conditional on no defaults in the waiting time\n",
    "        Y_proposed = cir_transition_sample_per_sector(Y_t, tau, kappa, theta, sigma, rng)\n",
    "\n",
    "        # compute proposed per-firm intensities (scalar values) using Y_proposed\n",
    "        per_firm_sys_prop = W[alive].dot(Y_proposed)\n",
    "        per_firm_total_prop = per_firm_sys_prop + per_firm_idio\n",
    "        lambda_proposed_total = per_firm_total_prop.sum()\n",
    "\n",
    "        # acceptance probability: Xi / Hepsilon where Xi = lambda_proposed_total\n",
    "        accept_prob = min(lambda_proposed_total / Hepsilon, 1.0)\n",
    "        if rng.random() < accept_prob:\n",
    "            # accept a default at t_candidate\n",
    "            t = t_candidate\n",
    "            events.append(t)\n",
    "\n",
    "            # choose which alive firm defaulted\n",
    "            probs = per_firm_total_prop / lambda_proposed_total\n",
    "            probs = np.maximum(probs, 0.0)\n",
    "            probs = probs / probs.sum()\n",
    "            selected_local_idx = rng.choice(len(alive), p=probs)\n",
    "            selected_firm = int(alive[selected_local_idx])\n",
    "\n",
    "            # record idiosyncratic factor\n",
    "            defaulter_idio_factor.append(float(idiosyncratic_factor[selected_firm]))\n",
    "\n",
    "            sector_contributions.append(W[selected_firm].copy())\n",
    "\n",
    "            # mark (loss) from contract exposure\n",
    "            contract = loan_contracts[selected_firm]\n",
    "            exposure = exposure_at_time(contract, T, t)\n",
    "            if exposure < 0:\n",
    "                warnings.warn(\"Exposure computed negative; clamping to zero.\")\n",
    "                exposure = max(exposure, 0.0)\n",
    "            if t >= T and exposure > 0:\n",
    "                warnings.warn(\"Exposure positive after maturity; forcing to zero.\")\n",
    "                exposure = 0.0\n",
    "\n",
    "            owed.append(exposure)\n",
    "            \n",
    "            LGD = rng.beta(alpha_lgd, beta_lgd)\n",
    "            mark = LGD * exposure\n",
    "            # mark =  exposure\n",
    "            if mark < 0:\n",
    "                warnings.warn(\"Loss mark negative; clamping to zero.\")\n",
    "                mark = max(mark, 0.0)\n",
    "            marks.append(mark)\n",
    "\n",
    "            # update Y_t with contagion\n",
    "            Y_t = Y_proposed + delta[selected_firm] * mark/contract[\"P\"]\n",
    "\n",
    "            # remove defaulted firm from alive set\n",
    "            alive = np.delete(alive, selected_local_idx)\n",
    "\n",
    "        else:\n",
    "            # reject, ie no default: advance time and set Y_t = Y_proposed\n",
    "            t = t_candidate\n",
    "            Y_t = Y_proposed\n",
    "\n",
    "    return (np.array(events), np.array(marks), np.array(owed),\n",
    "            np.array(defaulter_idio_factor), np.array(sector_contributions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d5bb5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_portfolio_weights(N, J, composition_type, concentration_params=None, rng=None):\n",
    "    \"\"\"\n",
    "    Build portfolio weights under three stylised configurations plus a random fallback.\n",
    "\n",
    "    - concentrated: one dominant sector for every obligor (eg for J=3 [0.7, 0.2, 0.1])\n",
    "    - balanced: evenly distributed across sectors (eg for J=3  [1/3, 1/3, 1/3])\n",
    "    - mixed: partial concentration with residual diversified weights (eg for J=3  [0.5, 0.3, 0.2])\n",
    "    - random: symmetric Dirichlet for variation.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    if concentration_params is None:\n",
    "        concentration_params = {}\n",
    "\n",
    "    noise_level = concentration_params.get(\"noise_level\", 0.0)\n",
    "    alpha_random = concentration_params.get(\"alpha\", 1.0)\n",
    "\n",
    "    if composition_type == \"concentrated\":\n",
    "        pattern = _pattern_concentrated(J)\n",
    "        W = _apply_noise_and_normalize(pattern, N, noise_level, rng)\n",
    "    elif composition_type == \"balanced\":\n",
    "        pattern = _pattern_balanced(J)\n",
    "        W = _apply_noise_and_normalize(pattern, N, noise_level, rng)\n",
    "    elif composition_type == \"mixed\":\n",
    "        pattern = _pattern_mixed(J)\n",
    "        W = _apply_noise_and_normalize(pattern, N, noise_level, rng)\n",
    "    elif composition_type == \"random\":\n",
    "        W = _generate_random_weights(N, J, {\"alpha\": alpha_random}, rng)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown composition_type: {composition_type}\")\n",
    "\n",
    "    return W\n",
    "\n",
    "\n",
    "def _pattern_concentrated(J, dominant=0.7):\n",
    "    if J <= 0:\n",
    "        return np.array([])\n",
    "    if J == 1:\n",
    "        return np.array([1.0])\n",
    "    residual = max(1.0 - dominant, 0.0)\n",
    "    tail = residual / (J - 1)\n",
    "    vec = np.full(J, tail)\n",
    "    vec[0] = dominant\n",
    "    return vec\n",
    "\n",
    "\n",
    "def _pattern_balanced(J):\n",
    "    if J <= 0:\n",
    "        return np.array([])\n",
    "    return np.full(J, 1.0 / J)\n",
    "\n",
    "\n",
    "def _pattern_mixed(J, primary=0.5, secondary=0.3):\n",
    "    if J <= 0:\n",
    "        return np.array([])\n",
    "    vec = np.zeros(J)\n",
    "    vec[0] = min(primary, 1.0)\n",
    "    if J >= 2:\n",
    "        vec[1] = min(secondary, max(1.0 - vec[0], 0.0))\n",
    "    residual = max(1.0 - vec.sum(), 0.0)\n",
    "    if J > 2:\n",
    "        vec[2:] = residual / (J - 2)\n",
    "    elif J == 1:\n",
    "        vec[0] = 1.0\n",
    "    else:\n",
    "        vec[1] += residual\n",
    "    return vec\n",
    "\n",
    "\n",
    "def _apply_noise_and_normalize(pattern, N, noise_level, rng):\n",
    "    base = np.tile(pattern, (N, 1))\n",
    "    if noise_level > 0:\n",
    "        noise = rng.normal(0.0, noise_level, size=base.shape)\n",
    "        base = base + noise\n",
    "    base = np.maximum(base, 1e-6)\n",
    "    base = base / base.sum(axis=1, keepdims=True)\n",
    "    return base\n",
    "\n",
    "\n",
    "def _generate_random_weights(N, J, params, rng):\n",
    "    alpha = params.get(\"alpha\", 1.0)\n",
    "    W = np.zeros((N, J))\n",
    "    for i in range(N):\n",
    "        W[i] = rng.dirichlet(np.full(J, alpha))\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41bb0b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(params, loan_contracts, num_trials, seed=42):\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    eps = 1e-5\n",
    "    losses = np.zeros(num_trials)\n",
    "    for i in range(num_trials):\n",
    "        events, marks, _, _, _ = simulate_sector_cir_model(params['kappa'], params['theta'], params['sigma'], \n",
    "                          params['T'], params['delta'], params['W'], 1e-5, \n",
    "                          params['lambda_benchmark'], \n",
    "                          params['Firms'], params['idio_factor'], \n",
    "                          loan_contracts, alpha_lgd, beta_lgd,\n",
    "                          rng)\n",
    "        losses[i] = np.sum(marks)\n",
    "    return losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
